{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "443e84e8-c2dd-4c10-aebd-a0ffb9fee629",
   "metadata": {},
   "source": [
    "# WE04 Universal Bank\n",
    "\n",
    "## Assignment Submitted by Aravind Dudam\n",
    "\n",
    "## U04440266"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8ac84c",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "26628a19-49f9-4ecf-b09d-fd0b0ff5f4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "# set random seed to ensure that results are repeatable\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefb0882",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f2d42c3c-f8fe-4568-81e5-28dec2b17cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "data = pd.read_csv(\"E:/DSP/Week 04/UniversalBank.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "80589521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of         ID  Age  Experience  Income  ZIP Code  Family  CCAvg  Education  \\\n",
       "0        1   25           1      49     91107       4    1.6          1   \n",
       "1        2   45          19      34     90089       3    1.5          1   \n",
       "2        3   39          15      11     94720       1    1.0          1   \n",
       "3        4   35           9     100     94112       1    2.7          2   \n",
       "4        5   35           8      45     91330       4    1.0          2   \n",
       "...    ...  ...         ...     ...       ...     ...    ...        ...   \n",
       "4995  4996   29           3      40     92697       1    1.9          3   \n",
       "4996  4997   30           4      15     92037       4    0.4          1   \n",
       "4997  4998   63          39      24     93023       2    0.3          3   \n",
       "4998  4999   65          40      49     90034       3    0.5          2   \n",
       "4999  5000   28           4      83     92612       3    0.8          1   \n",
       "\n",
       "      Mortgage  Personal Loan  Securities Account  CD Account  Online  \\\n",
       "0            0              0                   1           0       0   \n",
       "1            0              0                   1           0       0   \n",
       "2            0              0                   0           0       0   \n",
       "3            0              0                   0           0       0   \n",
       "4            0              0                   0           0       0   \n",
       "...        ...            ...                 ...         ...     ...   \n",
       "4995         0              0                   0           0       1   \n",
       "4996        85              0                   0           0       1   \n",
       "4997         0              0                   0           0       0   \n",
       "4998         0              0                   0           0       1   \n",
       "4999         0              0                   0           0       1   \n",
       "\n",
       "      CreditCard  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              1  \n",
       "...          ...  \n",
       "4995           0  \n",
       "4996           0  \n",
       "4997           0  \n",
       "4998           0  \n",
       "4999           1  \n",
       "\n",
       "[5000 rows x 14 columns]>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the data\n",
    "data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4a80822f-56dd-4b75-9a5e-f600ec8e791b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 14)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of rows and columns\n",
    "data.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dc9a3dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4698\n",
       "1     302\n",
       "Name: CD Account, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target Variable \"CD Account\"\n",
    "data['CD Account'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "260e0e3d-655a-44af-815f-20b86fb245e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 14 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   ID                  5000 non-null   int64  \n",
      " 1   Age                 5000 non-null   int64  \n",
      " 2   Experience          5000 non-null   int64  \n",
      " 3   Income              5000 non-null   int64  \n",
      " 4   ZIP Code            5000 non-null   int64  \n",
      " 5   Family              5000 non-null   int64  \n",
      " 6   CCAvg               5000 non-null   float64\n",
      " 7   Education           5000 non-null   int64  \n",
      " 8   Mortgage            5000 non-null   int64  \n",
      " 9   Personal Loan       5000 non-null   int64  \n",
      " 10  Securities Account  5000 non-null   int64  \n",
      " 11  CD Account          5000 non-null   int64  \n",
      " 12  Online              5000 non-null   int64  \n",
      " 13  CreditCard          5000 non-null   int64  \n",
      "dtypes: float64(1), int64(13)\n",
      "memory usage: 547.0 KB\n"
     ]
    }
   ],
   "source": [
    "# Identifying Nulls and Non Nulls\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3092771b",
   "metadata": {},
   "source": [
    "## Check the missing values with na's for each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f3b4e388-6fcc-4c45-a143-77d68b825bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                    0\n",
       "Age                   0\n",
       "Experience            0\n",
       "Income                0\n",
       "ZIP Code              0\n",
       "Family                0\n",
       "CCAvg                 0\n",
       "Education             0\n",
       "Mortgage              0\n",
       "Personal Loan         0\n",
       "Securities Account    0\n",
       "CD Account            0\n",
       "Online                0\n",
       "CreditCard            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aab10349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                    0\n",
       "Age                   0\n",
       "Experience            0\n",
       "Income                0\n",
       "ZIP Code              0\n",
       "Family                0\n",
       "CCAvg                 0\n",
       "Education             0\n",
       "Mortgage              0\n",
       "Personal Loan         0\n",
       "Securities Account    0\n",
       "CD Account            0\n",
       "Online                0\n",
       "CreditCard            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "24d1fec8-234f-46c6-b265-5365a92286bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CD Account            1.000000\n",
       "Securities Account    0.317034\n",
       "Personal Loan         0.316355\n",
       "CreditCard            0.278644\n",
       "Online                0.175880\n",
       "Income                0.169738\n",
       "CCAvg                 0.136534\n",
       "Mortgage              0.089311\n",
       "ZIP Code              0.019972\n",
       "Family                0.014110\n",
       "Education             0.013934\n",
       "Experience            0.010353\n",
       "Age                   0.008043\n",
       "ID                   -0.006909\n",
       "Name: CD Account, dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.corr()['CD Account'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8621b4",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d6e86953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Income</th>\n",
       "      <th>Family</th>\n",
       "      <th>CCAvg</th>\n",
       "      <th>Education</th>\n",
       "      <th>Mortgage</th>\n",
       "      <th>Personal Loan</th>\n",
       "      <th>Securities Account</th>\n",
       "      <th>CD Account</th>\n",
       "      <th>Online</th>\n",
       "      <th>CreditCard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>4996</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>4997</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>4998</td>\n",
       "      <td>63</td>\n",
       "      <td>39</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>4999</td>\n",
       "      <td>65</td>\n",
       "      <td>40</td>\n",
       "      <td>49</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>5000</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>83</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID  Age  Experience  Income  Family  CCAvg  Education  Mortgage  \\\n",
       "0        1   25           1      49       4    1.6          1         0   \n",
       "1        2   45          19      34       3    1.5          1         0   \n",
       "2        3   39          15      11       1    1.0          1         0   \n",
       "3        4   35           9     100       1    2.7          2         0   \n",
       "4        5   35           8      45       4    1.0          2         0   \n",
       "...    ...  ...         ...     ...     ...    ...        ...       ...   \n",
       "4995  4996   29           3      40       1    1.9          3         0   \n",
       "4996  4997   30           4      15       4    0.4          1        85   \n",
       "4997  4998   63          39      24       2    0.3          3         0   \n",
       "4998  4999   65          40      49       3    0.5          2         0   \n",
       "4999  5000   28           4      83       3    0.8          1         0   \n",
       "\n",
       "      Personal Loan  Securities Account  CD Account  Online  CreditCard  \n",
       "0                 0                   1           0       0           0  \n",
       "1                 0                   1           0       0           0  \n",
       "2                 0                   0           0       0           0  \n",
       "3                 0                   0           0       0           0  \n",
       "4                 0                   0           0       0           1  \n",
       "...             ...                 ...         ...     ...         ...  \n",
       "4995              0                   0           0       1           0  \n",
       "4996              0                   0           0       1           0  \n",
       "4997              0                   0           0       0           0  \n",
       "4998              0                   0           0       1           0  \n",
       "4999              0                   0           0       1           1  \n",
       "\n",
       "[5000 rows x 13 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(columns=[\"ID\"])\n",
    "data.drop(columns=[\"ZIP Code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f8158b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset is cleaned with the necessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "95c39ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing One hot encoding for preprocessing Education column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e4cdb1",
   "metadata": {},
   "source": [
    "## Using One-hot Encoding to encode the categorical variable \"Education\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "31bc56e3-f5d2-4589-8ac4-fa68229d368f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Income</th>\n",
       "      <th>ZIP Code</th>\n",
       "      <th>Family</th>\n",
       "      <th>CCAvg</th>\n",
       "      <th>Mortgage</th>\n",
       "      <th>Personal Loan</th>\n",
       "      <th>Securities Account</th>\n",
       "      <th>CD Account</th>\n",
       "      <th>Online</th>\n",
       "      <th>CreditCard</th>\n",
       "      <th>Education_1</th>\n",
       "      <th>Education_2</th>\n",
       "      <th>Education_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>91107</td>\n",
       "      <td>4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "      <td>90089</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>94720</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>94112</td>\n",
       "      <td>1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>91330</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>4996</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>92697</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>4997</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>92037</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>4998</td>\n",
       "      <td>63</td>\n",
       "      <td>39</td>\n",
       "      <td>24</td>\n",
       "      <td>93023</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>4999</td>\n",
       "      <td>65</td>\n",
       "      <td>40</td>\n",
       "      <td>49</td>\n",
       "      <td>90034</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>5000</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>83</td>\n",
       "      <td>92612</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID  Age  Experience  Income  ZIP Code  Family  CCAvg  Mortgage  \\\n",
       "0        1   25           1      49     91107       4    1.6         0   \n",
       "1        2   45          19      34     90089       3    1.5         0   \n",
       "2        3   39          15      11     94720       1    1.0         0   \n",
       "3        4   35           9     100     94112       1    2.7         0   \n",
       "4        5   35           8      45     91330       4    1.0         0   \n",
       "...    ...  ...         ...     ...       ...     ...    ...       ...   \n",
       "4995  4996   29           3      40     92697       1    1.9         0   \n",
       "4996  4997   30           4      15     92037       4    0.4        85   \n",
       "4997  4998   63          39      24     93023       2    0.3         0   \n",
       "4998  4999   65          40      49     90034       3    0.5         0   \n",
       "4999  5000   28           4      83     92612       3    0.8         0   \n",
       "\n",
       "      Personal Loan  Securities Account  CD Account  Online  CreditCard  \\\n",
       "0                 0                   1           0       0           0   \n",
       "1                 0                   1           0       0           0   \n",
       "2                 0                   0           0       0           0   \n",
       "3                 0                   0           0       0           0   \n",
       "4                 0                   0           0       0           1   \n",
       "...             ...                 ...         ...     ...         ...   \n",
       "4995              0                   0           0       1           0   \n",
       "4996              0                   0           0       1           0   \n",
       "4997              0                   0           0       0           0   \n",
       "4998              0                   0           0       1           0   \n",
       "4999              0                   0           0       1           1   \n",
       "\n",
       "      Education_1  Education_2  Education_3  \n",
       "0               1            0            0  \n",
       "1               1            0            0  \n",
       "2               1            0            0  \n",
       "3               0            1            0  \n",
       "4               0            1            0  \n",
       "...           ...          ...          ...  \n",
       "4995            0            0            1  \n",
       "4996            1            0            0  \n",
       "4997            0            0            1  \n",
       "4998            0            1            0  \n",
       "4999            1            0            0  \n",
       "\n",
       "[5000 rows x 16 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EncodedData = pd.get_dummies(data, columns = ['Education'])\n",
    "EncodedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6a849821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data split - train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7b9dc6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(data, test_size=0.3)\n",
    "target = 'CD Account'\n",
    "predictors = list(data.columns)\n",
    "predictors.remove(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "417c4053",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "cols_to_stdize = predictors               \n",
    "train_df[cols_to_stdize] = scaler.fit_transform(train_df[cols_to_stdize])\n",
    "test_df[cols_to_stdize] = scaler.transform(test_df[cols_to_stdize])\n",
    "\n",
    "train_X = train_df[predictors]\n",
    "train_y = train_df[target]\n",
    "test_X = train_df[predictors]\n",
    "test_y = test_df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ee5d0ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "model=scaler.fit(EncodedData)\n",
    "scaled_data=model.transform(EncodedData)\n",
    "\n",
    "scaled_data = pd.DataFrame(scaled_data, index=EncodedData.index, columns=EncodedData.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "91a62887-2bad-4115-8f6f-e4cbd9f08ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3500, 15), (1500, 15), (3500,), (1500,))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = scaled_data.drop('CD Account',axis=1)\n",
    "y = scaled_data['CD Account']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 14)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7c9928",
   "metadata": {},
   "source": [
    "# Modeling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94355180-cc3a-44a2-aab6-1791c374ef4b",
   "metadata": {},
   "source": [
    "## Linear regression with Randomsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ab464e8e-d75d-4979-855a-9e26134df6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best recall score is 0.6708245243128964\n",
      "... with parameters: {'solver': 'liblinear', 'penalty': 'l2', 'max_iter': 890, 'C': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aravi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "925 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "330 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aravi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aravi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\aravi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 464, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "355 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aravi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aravi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1471, in fit\n",
      "    raise ValueError(\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "240 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aravi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aravi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\aravi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 457, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aravi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.66627907        nan 0.66627907 0.66627907 0.67082452 0.31014799\n",
      "        nan        nan 0.66627907        nan 0.25549683        nan\n",
      " 0.05940803 0.66627907 0.66627907        nan 0.66627907 0.66627907\n",
      " 0.66627907        nan 0.37410148 0.                nan        nan\n",
      "        nan 0.66627907        nan 0.66627907 0.66627907 0.05940803\n",
      "        nan 0.05940803        nan        nan 0.66627907 0.37410148\n",
      " 0.66627907        nan 0.66627907 0.66627907 0.66627907 0.66627907\n",
      " 0.05940803 0.25549683 0.66627907 0.66627907 0.25549683 0.66627907\n",
      " 0.66627907        nan        nan 0.         0.66627907        nan\n",
      " 0.66627907        nan 0.66627907 0.37410148 0.66627907 0.66627907\n",
      " 0.66627907 0.66627907 0.67082452 0.66627907 0.66627907        nan\n",
      " 0.66627907        nan 0.66627907 0.05940803 0.66627907 0.37410148\n",
      " 0.05940803 0.                nan 0.66627907 0.66627907        nan\n",
      "        nan 0.                nan        nan        nan 0.66627907\n",
      " 0.67082452 0.         0.25549683 0.         0.25549683 0.05940803\n",
      " 0.66627907        nan 0.66627907        nan        nan 0.66627907\n",
      "        nan        nan        nan 0.66627907        nan 0.66627907\n",
      " 0.66627907 0.66627907 0.66627907 0.31014799 0.66627907 0.05940803\n",
      "        nan 0.66627907        nan 0.25549683        nan 0.66627907\n",
      "        nan 0.66627907        nan 0.66627907 0.66627907        nan\n",
      " 0.31014799 0.66627907 0.         0.66627907        nan        nan\n",
      "        nan 0.66627907 0.37410148 0.31014799 0.         0.05940803\n",
      " 0.66627907 0.37410148 0.66627907        nan 0.66627907 0.66627907\n",
      " 0.05940803 0.66627907 0.66627907 0.66627907 0.         0.66627907\n",
      "        nan        nan 0.66627907 0.66627907        nan        nan\n",
      " 0.05940803 0.66627907        nan        nan        nan        nan\n",
      " 0.66627907        nan 0.66627907        nan        nan        nan\n",
      " 0.31014799 0.67082452        nan 0.66627907 0.66627907        nan\n",
      "        nan        nan 0.66627907 0.37410148        nan 0.66627907\n",
      " 0.66627907 0.66627907        nan 0.66627907 0.66627907 0.66627907\n",
      "        nan        nan 0.31014799        nan 0.                nan\n",
      "        nan 0.66627907        nan 0.66627907 0.66627907 0.\n",
      " 0.66627907 0.66627907 0.66627907 0.37410148 0.66627907        nan\n",
      " 0.66627907 0.66627907        nan 0.37410148 0.31014799        nan\n",
      "        nan 0.66627907 0.66627907        nan 0.66627907 0.66627907\n",
      " 0.66627907        nan 0.66627907 0.05940803        nan        nan\n",
      " 0.66627907 0.25549683        nan        nan 0.37410148 0.66627907\n",
      "        nan        nan 0.66627907        nan 0.66627907        nan\n",
      " 0.25549683 0.67082452 0.66627907        nan 0.66627907 0.\n",
      " 0.67082452 0.66627907 0.66627907 0.66627907 0.67082452        nan\n",
      "        nan        nan 0.25549683        nan 0.66627907 0.66627907\n",
      "        nan 0.25549683        nan        nan        nan 0.66627907\n",
      " 0.66627907 0.66627907 0.66627907 0.66627907 0.67082452        nan\n",
      " 0.66627907        nan        nan 0.                nan 0.31014799\n",
      " 0.66627907 0.05940803        nan 0.66627907 0.                nan\n",
      " 0.66627907 0.                nan 0.66627907        nan 0.66627907\n",
      " 0.66627907        nan 0.66627907 0.66627907 0.67082452        nan\n",
      "        nan 0.66627907 0.         0.66627907        nan        nan\n",
      "        nan        nan        nan        nan 0.66627907 0.66627907\n",
      "        nan 0.66627907        nan 0.66627907 0.         0.66627907\n",
      "        nan 0.66627907        nan 0.05940803 0.05940803        nan\n",
      "        nan 0.66627907        nan        nan 0.31014799 0.66627907\n",
      " 0.37410148 0.66627907 0.66627907 0.66627907        nan        nan\n",
      "        nan 0.05940803 0.66627907        nan        nan 0.37410148\n",
      " 0.66627907 0.66627907 0.66627907 0.37410148        nan 0.66627907\n",
      " 0.66627907 0.66627907 0.05940803        nan        nan        nan\n",
      " 0.31014799 0.66627907 0.66627907 0.66627907 0.31014799 0.05940803\n",
      " 0.25549683 0.05940803        nan        nan 0.66627907 0.66627907\n",
      "        nan        nan 0.25549683 0.25549683 0.05940803 0.66627907\n",
      " 0.66627907 0.37410148        nan 0.66627907 0.66627907        nan\n",
      " 0.66627907 0.66627907 0.         0.66627907 0.         0.66627907\n",
      "        nan 0.66627907        nan 0.31014799 0.25549683 0.\n",
      "        nan 0.25549683        nan 0.67082452        nan        nan\n",
      " 0.66627907 0.         0.66627907        nan 0.67082452 0.66627907\n",
      " 0.                nan 0.66627907        nan 0.66627907 0.\n",
      "        nan        nan        nan        nan        nan 0.66627907\n",
      " 0.66627907 0.66627907 0.66627907 0.05940803        nan 0.37410148\n",
      " 0.37410148 0.66627907 0.66627907 0.66627907        nan        nan\n",
      "        nan 0.25549683        nan        nan        nan 0.66627907\n",
      " 0.66627907 0.         0.66627907        nan 0.66627907 0.66627907\n",
      "        nan 0.         0.66627907        nan 0.05940803 0.66627907\n",
      " 0.05940803        nan 0.37410148        nan 0.66627907        nan\n",
      "        nan 0.05940803        nan        nan 0.         0.25549683\n",
      " 0.66627907 0.66627907 0.31014799        nan        nan 0.31014799\n",
      " 0.37410148        nan 0.66627907 0.05940803        nan 0.31014799\n",
      " 0.         0.66627907 0.66627907        nan 0.31014799        nan\n",
      " 0.66627907        nan 0.         0.66627907 0.25549683 0.66627907\n",
      " 0.05940803        nan 0.05940803 0.66627907 0.66627907 0.66627907\n",
      "        nan 0.66627907        nan 0.66627907 0.         0.\n",
      " 0.                nan        nan        nan        nan 0.67082452\n",
      " 0.66627907        nan        nan 0.66627907        nan 0.66627907\n",
      " 0.         0.66627907        nan 0.66627907 0.66627907 0.66627907\n",
      "        nan        nan        nan        nan 0.66627907 0.05940803\n",
      "        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\aravi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.66664286        nan 0.66664286 0.66664286 0.66892857 0.31617532\n",
      "        nan        nan 0.66664286        nan 0.25338961        nan\n",
      " 0.05936364 0.66664286 0.66664286        nan 0.66664286 0.66664286\n",
      " 0.66664286        nan 0.42117532 0.                nan        nan\n",
      "        nan 0.66664286        nan 0.66664286 0.66664286 0.05936364\n",
      "        nan 0.05936364        nan        nan 0.66664286 0.42117532\n",
      " 0.66664286        nan 0.66664286 0.66664286 0.66664286 0.66664286\n",
      " 0.05936364 0.25338961 0.66664286 0.66664286 0.25338961 0.66664286\n",
      " 0.66664286        nan        nan 0.         0.66664286        nan\n",
      " 0.66664286        nan 0.66664286 0.42117532 0.66664286 0.66664286\n",
      " 0.66664286 0.66664286 0.66892857 0.66664286 0.66664286        nan\n",
      " 0.66664286        nan 0.66664286 0.05936364 0.66664286 0.42117532\n",
      " 0.05936364 0.                nan 0.66664286 0.66664286        nan\n",
      "        nan 0.                nan        nan        nan 0.66664286\n",
      " 0.66892857 0.         0.25338961 0.         0.25338961 0.05936364\n",
      " 0.66778571        nan 0.66664286        nan        nan 0.66664286\n",
      "        nan        nan        nan 0.66664286        nan 0.66664286\n",
      " 0.66664286 0.66664286 0.66664286 0.31617532 0.66664286 0.05936364\n",
      "        nan 0.66664286        nan 0.25338961        nan 0.66664286\n",
      "        nan 0.66664286        nan 0.66778571 0.66664286        nan\n",
      " 0.31617532 0.66664286 0.         0.66664286        nan        nan\n",
      "        nan 0.66664286 0.42117532 0.31617532 0.         0.05936364\n",
      " 0.66664286 0.42117532 0.66664286        nan 0.66664286 0.66664286\n",
      " 0.05936364 0.66664286 0.66664286 0.66664286 0.         0.66664286\n",
      "        nan        nan 0.66664286 0.66664286        nan        nan\n",
      " 0.05936364 0.66664286        nan        nan        nan        nan\n",
      " 0.66664286        nan 0.66664286        nan        nan        nan\n",
      " 0.31617532 0.66892857        nan 0.66664286 0.66664286        nan\n",
      "        nan        nan 0.66664286 0.42117532        nan 0.66664286\n",
      " 0.66664286 0.66664286        nan 0.66664286 0.66664286 0.66664286\n",
      "        nan        nan 0.31617532        nan 0.                nan\n",
      "        nan 0.66664286        nan 0.66664286 0.66664286 0.\n",
      " 0.66664286 0.66778571 0.66664286 0.42117532 0.66664286        nan\n",
      " 0.66664286 0.66664286        nan 0.42117532 0.31617532        nan\n",
      "        nan 0.66664286 0.66778571        nan 0.66664286 0.66664286\n",
      " 0.66664286        nan 0.66664286 0.05936364        nan        nan\n",
      " 0.66664286 0.25338961        nan        nan 0.42117532 0.66664286\n",
      "        nan        nan 0.66664286        nan 0.66664286        nan\n",
      " 0.25338961 0.66892857 0.66664286        nan 0.66664286 0.\n",
      " 0.66892857 0.66664286 0.66664286 0.66664286 0.66892857        nan\n",
      "        nan        nan 0.25338961        nan 0.66664286 0.66664286\n",
      "        nan 0.25338961        nan        nan        nan 0.66664286\n",
      " 0.66664286 0.66664286 0.66664286 0.66664286 0.66892857        nan\n",
      " 0.66664286        nan        nan 0.                nan 0.31617532\n",
      " 0.66664286 0.05936364        nan 0.66664286 0.                nan\n",
      " 0.66664286 0.                nan 0.66664286        nan 0.66664286\n",
      " 0.66664286        nan 0.66664286 0.66664286 0.66892857        nan\n",
      "        nan 0.66664286 0.         0.66664286        nan        nan\n",
      "        nan        nan        nan        nan 0.66664286 0.66664286\n",
      "        nan 0.66664286        nan 0.66664286 0.         0.66664286\n",
      "        nan 0.66664286        nan 0.05936364 0.05936364        nan\n",
      "        nan 0.66664286        nan        nan 0.31617532 0.66664286\n",
      " 0.42117532 0.66664286 0.66778571 0.66664286        nan        nan\n",
      "        nan 0.05936364 0.66664286        nan        nan 0.42117532\n",
      " 0.66664286 0.66664286 0.66664286 0.42117532        nan 0.66664286\n",
      " 0.66664286 0.66664286 0.05936364        nan        nan        nan\n",
      " 0.31617532 0.66664286 0.66664286 0.66664286 0.31617532 0.05936364\n",
      " 0.25338961 0.05936364        nan        nan 0.66664286 0.66664286\n",
      "        nan        nan 0.25338961 0.25338961 0.05936364 0.66664286\n",
      " 0.66664286 0.42117532        nan 0.66778571 0.66664286        nan\n",
      " 0.66664286 0.66664286 0.         0.66664286 0.         0.66664286\n",
      "        nan 0.66664286        nan 0.31617532 0.25338961 0.\n",
      "        nan 0.25338961        nan 0.66892857        nan        nan\n",
      " 0.66664286 0.         0.66664286        nan 0.66892857 0.66664286\n",
      " 0.                nan 0.66664286        nan 0.66664286 0.\n",
      "        nan        nan        nan        nan        nan 0.66664286\n",
      " 0.66664286 0.66664286 0.66664286 0.05936364        nan 0.42117532\n",
      " 0.42117532 0.66664286 0.66664286 0.66664286        nan        nan\n",
      "        nan 0.25338961        nan        nan        nan 0.66778571\n",
      " 0.66664286 0.         0.66664286        nan 0.66664286 0.66664286\n",
      "        nan 0.         0.66664286        nan 0.05936364 0.66664286\n",
      " 0.05936364        nan 0.42117532        nan 0.66664286        nan\n",
      "        nan 0.05936364        nan        nan 0.         0.25338961\n",
      " 0.66664286 0.66664286 0.31617532        nan        nan 0.31617532\n",
      " 0.42117532        nan 0.66664286 0.05936364        nan 0.31617532\n",
      " 0.         0.66664286 0.66664286        nan 0.31617532        nan\n",
      " 0.66664286        nan 0.         0.66664286 0.25338961 0.66664286\n",
      " 0.05936364        nan 0.05936364 0.66664286 0.66664286 0.66664286\n",
      "        nan 0.66664286        nan 0.66664286 0.         0.\n",
      " 0.                nan        nan        nan        nan 0.66892857\n",
      " 0.66664286        nan        nan 0.66664286        nan 0.66664286\n",
      " 0.         0.66664286        nan 0.66664286 0.66664286 0.66664286\n",
      "        nan        nan        nan        nan 0.66664286 0.05936364\n",
      "        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {'C':[0.001,0.01,0.1,1,10], # C is the regulization strength\n",
    "               'penalty':['l1', 'l2','elasticnet','none'],\n",
    "              'solver':['saga','liblinear'],\n",
    "              'max_iter': np.arange(500,1000)\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "lr = LogisticRegression()\n",
    "rand_search = RandomizedSearchCV(estimator = lr, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(train_X,train_y)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestlr = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e4d421-b833-4a13-87e2-a555e4a90232",
   "metadata": {},
   "source": [
    "## Logistic regression using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "454d34da-fbeb-4f4b-b4d0-2209322df788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1600 candidates, totalling 8000 fits\n",
      "The best recall score is 0.6708245243128964\n",
      "... with parameters: {'C': 0.09999999999999998, 'max_iter': 490, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aravi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "4000 fits failed out of a total of 8000.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "4000 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aravi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aravi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1464, in fit\n",
      "    raise ValueError(\"Penalty term must be positive; got (C=%r)\" % self.C)\n",
      "ValueError: Penalty term must be positive; got (C=-0.9)\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aravi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan ... 0.67082452 0.67082452 0.67082452]\n",
      "  warnings.warn(\n",
      "C:\\Users\\aravi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [       nan        nan        nan ... 0.66892857 0.66892857 0.66892857]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "best_penality = rand_search.best_params_['penalty']\n",
    "best_solver = rand_search.best_params_['solver']\n",
    "min_regulization_strength=rand_search.best_params_['C']\n",
    "min_iter = rand_search.best_params_['max_iter']\n",
    "\n",
    "#Using the best parameters from the Random Search to use as range for the parameters to do the grid search\n",
    "param_grid = {\n",
    "    \n",
    "    'C':np.arange(min_regulization_strength-1,min_regulization_strength+1), \n",
    "               'penalty':[best_penality],\n",
    "              'solver':[best_solver],\n",
    "              'max_iter': np.arange(min_iter-400,min_iter+400)\n",
    "}\n",
    "\n",
    "logreg =  LogisticRegression()\n",
    "grid_search = GridSearchCV(estimator = logreg, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1, # n_jobs=-1 will utilize all available CPUs \n",
    "                return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(train_X,train_y)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestlogreg = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd76d26c-b5a2-43d8-b788-75e962977625",
   "metadata": {},
   "source": [
    "## SVM classification model using Random Search for Linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "98bf73fd-b6ce-47c0-8671-25d69bfb8def",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aravi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 7 is smaller than n_iter=500. Running 7 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "The best recall score is 0.6691056910569106\n",
      "... with parameters: {'kernel': 'linear', 'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "     'C': [0.0001, 0.001, 0.1, 1, 10, 100, 1000], \n",
    "    'kernel': ['linear']\n",
    "}\n",
    "\n",
    "random_linear_SVC = SVC()\n",
    "random_search = RandomizedSearchCV(estimator = random_linear_SVC, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = random_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {random_search.best_score_}\")\n",
    "print(f\"... with parameters: {random_search.best_params_}\")\n",
    "\n",
    "bestPrecisionTree = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2216200-252f-40b2-be57-0ccb8bdcc4c3",
   "metadata": {},
   "source": [
    "## SVM classification model using Grid Search for Linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "49ae62cb-4141-4004-9b01-f68c07a0c732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "The best recall score is 0.6691056910569106\n",
      "... with parameters: {'C': 0.1, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "     'C': [0.0001, 0.001, 0.1, 1, 10, 100, 1000], \n",
    "    'kernel': ['linear']\n",
    "}\n",
    "\n",
    "grid_linear_SVC = SVC()\n",
    "grid_search = GridSearchCV(estimator = grid_linear_SVC, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestPrecisionTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d749cfb",
   "metadata": {},
   "source": [
    "## SVM classification model using Random Search for Poly kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e896a19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aravi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 16 is smaller than n_iter=500. Running 16 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "The best recall score is 0.6980255516840883\n",
      "... with parameters: {'kernel': 'poly', 'gamma': 1, 'C': 10}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "     'C': [0.1,1, 10, 100], \n",
    "    'gamma': [1,0.1,0.01,0.001],\n",
    "    'kernel': ['poly']\n",
    "}\n",
    "\n",
    "random_Poly_SVC = SVC()\n",
    "random_search = RandomizedSearchCV(estimator = random_Poly_SVC, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = random_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {random_search.best_score_}\")\n",
    "print(f\"... with parameters: {random_search.best_params_}\")\n",
    "\n",
    "bestPrecisionTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b44648",
   "metadata": {},
   "source": [
    "## SVM classification model using Grid Search for Poly kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c39b664f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "The best recall score is 0.6980255516840883\n",
      "... with parameters: {'C': 10, 'gamma': 1, 'kernel': 'poly'}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "     'C': [0.1,1, 10, 100], \n",
    "    'gamma': [1,0.1,0.01,0.001],\n",
    "    'kernel': ['poly']\n",
    "}\n",
    "\n",
    "\n",
    "grid_Poly_SVC = SVC()\n",
    "grid_search = GridSearchCV(estimator = grid_Poly_SVC, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestPrecisionTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8e0cc8",
   "metadata": {},
   "source": [
    "## SVM classification model using Random Search for rbf kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2c628d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aravi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 9 is smaller than n_iter=500. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n",
      "The best recall score is 0.6688095238095239\n",
      "... with parameters: {'kernel': 'rbf', 'gamma': 0.1, 'C': 1}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 10\n",
    "\n",
    "param_grid = {\n",
    "     'C': [0.1,1, 10], \n",
    "    'gamma': [1,0.1,0.011],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "random_rbf_SVC = SVC()\n",
    "random_search = RandomizedSearchCV(estimator = random_rbf_SVC, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = random_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {random_search.best_score_}\")\n",
    "print(f\"... with parameters: {random_search.best_params_}\")\n",
    "\n",
    "bestPrecisionTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da744a15",
   "metadata": {},
   "source": [
    "## SVM classification model using Grid Search for rbf kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "24e97116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "The best recall score is 0.6691056910569106\n",
      "... with parameters: {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "     'C': [0.1,1, 10], \n",
    "    'gamma': [1,0.1,0.011],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "grid_rbf_SVC = SVC()\n",
    "grid_search = GridSearchCV(estimator = grid_rbf_SVC, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestPrecisionTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fc230f",
   "metadata": {},
   "source": [
    "## Random search for the Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9954e7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best recall score is 0.7170731707317073\n",
      "... with parameters: {'min_samples_split': 99, 'min_samples_leaf': 4, 'min_impurity_decrease': 0.0036, 'max_leaf_nodes': 93, 'max_depth': 7, 'criterion': 'entropy'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aravi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "15 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aravi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aravi\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\aravi\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aravi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.65493612 0.         0.3203252  0.66434379 0.65493612 0.50708479\n",
      " 0.65493612 0.34889663 0.4445993  0.25365854 0.4398374  0.\n",
      " 0.08095238 0.33031359 0.58327526 0.34889663 0.12508711 0.58327526\n",
      " 0.34889663 0.13461092 0.37746806 0.46840883 0.12508711 0.65493612\n",
      " 0.58327526 0.58327526 0.70278746 0.4398374  0.34889663 0.55470383\n",
      " 0.34889663 0.66445993 0.46840883 0.66445993 0.31080139 0.13461092\n",
      " 0.         0.08095238 0.67421603 0.54041812 0.40650407 0.12508711\n",
      " 0.08095238 0.5213705  0.71707317 0.66445993 0.71707317 0.58327526\n",
      " 0.5213705  0.4398374  0.46898955 0.34889663 0.34889663 0.5213705\n",
      " 0.34889663 0.34889663 0.56898955 0.58327526 0.34889663 0.34889663\n",
      " 0.12508711 0.4445993  0.67421603 0.65493612 0.71707317 0.08095238\n",
      " 0.34889663 0.20603949 0.65493612 0.20603949 0.58327526 0.34889663\n",
      " 0.13461092 0.34889663 0.65493612 0.20603949 0.12508711 0.65493612\n",
      " 0.08095238 0.58327526 0.         0.3203252  0.5213705  0.34889663\n",
      " 0.67421603 0.66445993 0.58327526 0.33461092 0.65493612 0.37746806\n",
      " 0.55470383 0.34889663 0.47851336 0.05365854 0.05365854 0.34889663\n",
      " 0.58327526 0.64529617 0.58327526 0.25365854 0.58327526 0.08095238\n",
      " 0.58327526 0.34889663 0.34889663 0.08095238 0.34889663 0.66445993\n",
      " 0.34889663 0.65493612 0.48757259 0.37746806 0.66445993 0.34889663\n",
      " 0.34889663 0.56898955 0.46898955 0.67421603 0.34889663 0.08095238\n",
      " 0.58327526 0.58327526 0.5213705  0.55993031 0.55470383 0.20603949\n",
      " 0.71707317 0.33461092 0.13461092 0.58327526 0.         0.65493612\n",
      " 0.58327526 0.34889663 0.58327526 0.34889663 0.65493612 0.34889663\n",
      " 0.37746806 0.65493612 0.37746806 0.34889663 0.         0.33461092\n",
      " 0.08095238 0.65493612 0.34889663 0.56898955 0.67421603 0.56898955\n",
      " 0.65516841 0.3203252  0.58327526 0.25365854 0.54041812 0.37746806\n",
      " 0.33461092 0.55470383 0.58327526 0.34889663 0.25365854 0.66445993\n",
      " 0.08095238 0.58327526 0.46840883 0.08095238 0.58327526 0.65493612\n",
      " 0.08095238 0.34889663 0.34889663 0.34889663 0.55993031 0.33461092\n",
      " 0.66445993 0.34889663 0.65493612 0.34889663 0.55470383 0.5213705\n",
      " 0.66445993 0.         0.54041812 0.34889663 0.56898955 0.56898955\n",
      " 0.65493612 0.67421603 0.37746806 0.20603949 0.56898955 0.20603949\n",
      " 0.         0.37746806 0.65005807 0.37746806 0.34889663 0.65493612\n",
      " 0.4398374  0.37793264 0.55470383 0.56898955 0.65493612 0.5213705\n",
      " 0.55470383 0.34889663 0.66445993 0.34889663 0.33461092 0.37746806\n",
      " 0.4445993  0.65493612 0.65493612 0.34889663 0.34889663 0.34889663\n",
      " 0.71707317 0.5213705  0.08095238 0.37746806 0.46840883 0.66445993\n",
      " 0.67421603 0.58327526 0.34889663 0.58327526 0.08095238 0.33461092\n",
      " 0.54041812 0.34889663 0.67421603 0.58327526 0.         0.65493612\n",
      " 0.34889663 0.65493612 0.56898955 0.20603949 0.65493612 0.5213705\n",
      " 0.08095238 0.12508711 0.71707317 0.67421603 0.08095238 0.12508711\n",
      " 0.58327526 0.34889663 0.         0.58327526 0.37746806 0.\n",
      " 0.34889663 0.34889663 0.58327526 0.66445993 0.4398374  0.34889663\n",
      " 0.34889663 0.34889663 0.65493612 0.56898955 0.33461092 0.58327526\n",
      " 0.34889663 0.37746806 0.67421603 0.67421603 0.25365854 0.23461092\n",
      " 0.71707317 0.67421603 0.71707317 0.34889663 0.4398374  0.46840883\n",
      " 0.65493612 0.34889663 0.34889663 0.58327526 0.66445993 0.66445993\n",
      " 0.4398374  0.4398374  0.34889663 0.65493612 0.34889663 0.\n",
      " 0.65493612 0.34889663 0.42078978 0.67421603 0.6456446  0.13461092\n",
      " 0.55470383 0.20603949 0.08095238 0.34889663 0.4398374  0.58327526\n",
      " 0.58327526 0.56898955 0.33461092 0.34889663 0.54041812 0.71707317\n",
      " 0.58327526 0.67421603 0.66445993 0.34889663 0.33461092 0.37746806\n",
      " 0.         0.34889663 0.71707317 0.08095238 0.34889663 0.67421603\n",
      " 0.58327526 0.34889663 0.65493612 0.34889663 0.13461092 0.58327526\n",
      " 0.65493612 0.20603949 0.45412311 0.43507549 0.71707317 0.34889663\n",
      " 0.56898955 0.33461092        nan 0.37746806 0.58327526 0.34889663\n",
      " 0.37746806 0.69291521 0.46898955 0.54041812 0.34889663 0.37746806\n",
      " 0.3203252  0.12508711 0.5213705  0.34889663 0.58327526 0.49279907\n",
      " 0.65005807 0.34889663 0.34889663 0.55470383 0.71707317 0.12508711\n",
      " 0.56898955 0.67421603 0.4398374  0.58327526 0.34889663 0.58327526\n",
      " 0.34889663 0.58327526 0.58327526 0.34889663 0.55993031 0.36318235\n",
      " 0.65516841 0.58327526 0.5213705  0.34889663 0.67421603 0.34889663\n",
      " 0.60255517 0.56898955 0.         0.34889663 0.58327526 0.65493612\n",
      " 0.71707317 0.34889663 0.34889663 0.         0.46840883 0.67421603\n",
      " 0.64529617 0.4398374  0.58327526 0.5213705  0.4398374  0.58327526\n",
      " 0.56898955 0.70278746 0.08095238 0.4445993  0.71707317 0.34889663\n",
      " 0.34889663 0.65493612 0.71707317 0.58327526 0.55470383 0.20603949\n",
      " 0.34889663 0.34889663 0.25365854 0.56898955        nan 0.48269454\n",
      " 0.66445993 0.48269454 0.         0.34889663 0.58327526 0.34889663\n",
      " 0.56898955 0.67421603 0.25365854 0.34889663 0.20603949 0.58327526\n",
      " 0.58327526 0.4398374  0.34889663 0.67421603 0.08095238 0.13461092\n",
      " 0.5213705  0.58327526 0.71707317 0.67421603 0.45412311 0.4398374\n",
      " 0.65493612 0.3398374  0.5213705  0.37746806 0.34889663        nan\n",
      " 0.70278746 0.58327526 0.34889663 0.37746806 0.67421603 0.20603949\n",
      " 0.58327526 0.65493612 0.6358885  0.37746806 0.58327526 0.48269454\n",
      " 0.34889663 0.58327526 0.58327526 0.46840883 0.34889663 0.65493612\n",
      " 0.67421603 0.         0.30603949 0.12508711 0.67421603 0.67421603\n",
      " 0.34889663 0.58327526 0.5213705  0.67421603 0.60255517 0.37746806\n",
      " 0.67421603 0.33461092 0.08095238 0.65493612 0.65493612 0.58327526\n",
      " 0.34889663 0.38269454 0.65493612 0.36318235 0.33461092 0.58327526\n",
      " 0.37746806 0.33461092 0.5213705  0.46840883 0.13461092 0.12508711\n",
      " 0.58327526 0.45412311 0.34889663 0.34889663 0.46840883 0.58327526\n",
      " 0.58327526 0.34889663 0.58327526 0.34889663 0.56898955 0.71707317\n",
      " 0.13461092 0.4445993 ]\n",
      "  warnings.warn(\n",
      "C:\\Users\\aravi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.65546764 0.         0.32417308 0.69131024 0.65546764 0.52269033\n",
      " 0.65546764 0.34925863 0.4497505  0.28578557 0.44016966 0.\n",
      " 0.06347305 0.34436128 0.58370402 0.34925863 0.1432706  0.58370402\n",
      " 0.34925863 0.13608497 0.40075563 0.48687625 0.1432706  0.65546764\n",
      " 0.58370402 0.58370402 0.69976476 0.44016966 0.34925863 0.55861848\n",
      " 0.34925863 0.67817936 0.48687625 0.65786285 0.31578985 0.13608497\n",
      " 0.         0.06347305 0.67461506 0.54065441 0.43537924 0.1432706\n",
      " 0.06347305 0.54059025 0.71766467 0.65786285 0.71766467 0.57298973\n",
      " 0.54059025 0.44016966 0.49508127 0.34925863 0.34925863 0.54059025\n",
      " 0.34925863 0.34925863 0.56580411 0.58370402 0.34925863 0.34925863\n",
      " 0.1432706  0.4497505  0.66982464 0.65546764 0.71766467 0.06347305\n",
      " 0.34925863 0.20554605 0.65546764 0.20554605 0.58370402 0.34925863\n",
      " 0.13608497 0.34925863 0.65546764 0.20554605 0.1432706  0.65546764\n",
      " 0.06347305 0.58370402 0.         0.32417308 0.54059025 0.34925863\n",
      " 0.66982464 0.66740091 0.58370402 0.34806102 0.65546764 0.40075563\n",
      " 0.55861848 0.34925863 0.49754063 0.07380952 0.07380952 0.34925863\n",
      " 0.58370402 0.64350585 0.58370402 0.28578557 0.58370402 0.06347305\n",
      " 0.58370402 0.34925863 0.34925863 0.06347305 0.34925863 0.65786285\n",
      " 0.34925863 0.65546764 0.50363559 0.40075563 0.65786285 0.34925863\n",
      " 0.34925863 0.56580411 0.49508127 0.66982464 0.34925863 0.06347305\n",
      " 0.58370402 0.58370402 0.54059025 0.56922583 0.55861848 0.20554605\n",
      " 0.70695038 0.34806102 0.13608497 0.58370402 0.         0.6710365\n",
      " 0.58370402 0.34925863 0.58370402 0.34925863 0.65546764 0.34925863\n",
      " 0.40075563 0.65546764 0.40075563 0.37567009 0.         0.34806102\n",
      " 0.06347305 0.65546764 0.34925863 0.56580411 0.67461506 0.56580411\n",
      " 0.66616766 0.32417308 0.58370402 0.28578557 0.54065441 0.40075563\n",
      " 0.33135871 0.55861848 0.58370402 0.34925863 0.28578557 0.66265326\n",
      " 0.06347305 0.58370402 0.48687625 0.06347305 0.58370402 0.65546764\n",
      " 0.06347305 0.33854434 0.34925863 0.34925863 0.56922583 0.33135871\n",
      " 0.65786285 0.34925863 0.65546764 0.34925863 0.55861848 0.54059025\n",
      " 0.65786285 0.         0.54065441 0.34925863 0.56580411 0.56580411\n",
      " 0.67338894 0.66982464 0.40075563 0.20554605 0.56580411 0.20554605\n",
      " 0.         0.40075563 0.67100798 0.39004135 0.34925863 0.65546764\n",
      " 0.44016966 0.38867265 0.55861848 0.56580411 0.65546764 0.54059025\n",
      " 0.54784003 0.34925863 0.65786285 0.34925863 0.34806102 0.40075563\n",
      " 0.4497505  0.65546764 0.65546764 0.34925863 0.34925863 0.34925863\n",
      " 0.72005988 0.54059025 0.06347305 0.40075563 0.48687625 0.65786285\n",
      " 0.67461506 0.58370402 0.34925863 0.57298973 0.06347305 0.34806102\n",
      " 0.54065441 0.34925863 0.67461506 0.57298973 0.         0.65546764\n",
      " 0.34925863 0.65546764 0.56580411 0.20554605 0.65546764 0.54059025\n",
      " 0.06347305 0.1432706  0.70695038 0.67461506 0.06347305 0.1432706\n",
      " 0.58370402 0.34925863 0.         0.58370402 0.40075563 0.\n",
      " 0.34925863 0.34925863 0.58370402 0.65786285 0.44016966 0.34925863\n",
      " 0.34925863 0.34925863 0.65546764 0.56580411 0.34806102 0.58370402\n",
      " 0.34925863 0.40075563 0.67461506 0.67461506 0.28578557 0.25345024\n",
      " 0.71766467 0.67461506 0.70695038 0.34925863 0.46052894 0.46531936\n",
      " 0.65546764 0.34925863 0.34925863 0.58370402 0.65786285 0.65786285\n",
      " 0.44016966 0.46531936 0.34925863 0.65546764 0.34925863 0.\n",
      " 0.65546764 0.34925863 0.43172227 0.67461506 0.64826775 0.13608497\n",
      " 0.55861848 0.20554605 0.06347305 0.34925863 0.44016966 0.58370402\n",
      " 0.57298973 0.56580411 0.34806102 0.34925863 0.54065441 0.71766467\n",
      " 0.58370402 0.67461506 0.65786285 0.34925863 0.34806102 0.40075563\n",
      " 0.         0.34925863 0.71766467 0.06347305 0.34925863 0.67461506\n",
      " 0.58370402 0.34925863 0.65546764 0.34925863 0.13608497 0.58370402\n",
      " 0.65546764 0.20554605 0.45813373 0.44016966 0.70695038 0.34925863\n",
      " 0.56580411 0.34806102        nan 0.40075563 0.58370402 0.34925863\n",
      " 0.40075563 0.79180211 0.49508127 0.54065441 0.34925863 0.40075563\n",
      " 0.32417308 0.1432706  0.54059025 0.34925863 0.58370402 0.5155047\n",
      " 0.6746008  0.34925863 0.34925863 0.55861848 0.71766467 0.1432706\n",
      " 0.56580411 0.67461506 0.44016966 0.58370402 0.34925863 0.58370402\n",
      " 0.34925863 0.57298973 0.58370402 0.34925863 0.56922583 0.38285572\n",
      " 0.65545338 0.58370402 0.54059025 0.37567009 0.67461506 0.34925863\n",
      " 0.61474907 0.56580411 0.         0.34925863 0.58370402 0.65546764\n",
      " 0.70695038 0.34925863 0.34925863 0.         0.46531936 0.67461506\n",
      " 0.65069147 0.44016966 0.58370402 0.54059025 0.44016966 0.58370402\n",
      " 0.56580411 0.69976476 0.06347305 0.4497505  0.70695038 0.34925863\n",
      " 0.34925863 0.65546764 0.71766467 0.58370402 0.55861848 0.20554605\n",
      " 0.34925863 0.34925863 0.28578557 0.56580411        nan 0.47250499\n",
      " 0.65786285 0.47250499 0.         0.34925863 0.58370402 0.34925863\n",
      " 0.56580411 0.67461506 0.28578557 0.34925863 0.20554605 0.58370402\n",
      " 0.58370402 0.44016966 0.34925863 0.67461506 0.06347305 0.13608497\n",
      " 0.54059025 0.58370402 0.70695038 0.67461506 0.45813373 0.44016966\n",
      " 0.65546764 0.34196607 0.54059025 0.40075563 0.34925863        nan\n",
      " 0.69976476 0.58370402 0.34925863 0.40075563 0.66982464 0.20554605\n",
      " 0.58370402 0.65546764 0.63151554 0.40075563 0.58370402 0.48321928\n",
      " 0.34925863 0.58370402 0.58370402 0.48687625 0.34925863 0.65546764\n",
      " 0.67461506 0.         0.30620901 0.1432706  0.67461506 0.66982464\n",
      " 0.34925863 0.58370402 0.54059025 0.66982464 0.61474907 0.40075563\n",
      " 0.67461506 0.34806102 0.06347305 0.65546764 0.65546764 0.58370402\n",
      " 0.34925863 0.39825349 0.65546764 0.38285572 0.35770602 0.58370402\n",
      " 0.40075563 0.34806102 0.54059025 0.46531936 0.13608497 0.1432706\n",
      " 0.58370402 0.45813373 0.34925863 0.34925863 0.48687625 0.58370402\n",
      " 0.58370402 0.34925863 0.58370402 0.34925863 0.56580411 0.71766467\n",
      " 0.13608497 0.4497505 ]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,100),  \n",
    "    'min_samples_leaf': np.arange(1,100),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 100), \n",
    "    'max_depth': np.arange(1,50), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83298414",
   "metadata": {},
   "source": [
    "## Grid search for the Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2f3b3c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5040 candidates, totalling 25200 fits\n",
      "The best recall score is 0.7170731707317073\n",
      "... with parameters: {'criterion': 'entropy', 'max_depth': 38, 'max_leaf_nodes': 92, 'min_impurity_decrease': 0.0038, 'min_samples_leaf': 9, 'min_samples_split': 94}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(94,99),  \n",
    "    'min_samples_leaf': np.arange(9,13),\n",
    "    'min_impurity_decrease': np.arange(0.0038, 0.0044, 0.0001),\n",
    "    'max_leaf_nodes': np.arange(92,98), \n",
    "    'max_depth': np.arange(38,44), \n",
    "    'criterion': ['entropy'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator = dtree, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b455964d",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "From the above results, here are the best recall scores determined:\n",
    "    \n",
    "    1. Logistic Regression with the Random-Search search 67.1%\n",
    "    2. Logistic Regression with the Grid search 67.1%\n",
    "    3. SVM classification model using Random Search for Linear kernel is 66.9%\n",
    "    4. SVM classification model using Grid Search for Linear kernel is 66.9%\n",
    "    5. SVM classification model using Random Search for Poly kernel is 69.8%\n",
    "    6. SVM classification model using Grid Search for Poly kernel is 69.8%\n",
    "    7. SVM classification model using Random Search for rbf kernel is 66.8%\n",
    "    8. SVM classification model using Grid Search for rbf kernel is 66.9%\n",
    "    9. Random search for the Decision Tree Classifier is 71.7%\n",
    "    10. Grid search for the Decision Tree Classifier is 71.7%\n",
    "    \n",
    "We have all of the values for the decision tree classifier with the optimal parameters for each model examined, including those for grid search, random search, and SVM Classification model employing three kernels (Linera, Poly, and rbf). This allows us to conclude that the decision tree classifier, for both randomSearch CV and GridSearch CV, has the best recall score, which is 71.7%. We may conclude that the decision tree classifier is the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9da4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
